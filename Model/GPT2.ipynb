{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2cvSUikEJOL3",
    "outputId": "3cd0a120-2574-49e3-ad31-857da2594dd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.28.0 in c:\\users\\parkchunsoo\\anaconda3\\lib\\site-packages (4.28.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\parkchunsoo\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (21.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\parkchunsoo\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (1.22.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\parkchunsoo\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (3.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\parkchunsoo\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (4.62.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\parkchunsoo\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\parkchunsoo\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (2021.8.3)\n",
      "Requirement already satisfied: requests in c:\\users\\parkchunsoo\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (2.26.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\parkchunsoo\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (0.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\parkchunsoo\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (0.14.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\parkchunsoo\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (3.10.0.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\parkchunsoo\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (2023.6.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\parkchunsoo\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers==4.28.0) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\parkchunsoo\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers==4.28.0) (0.4.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\parkchunsoo\\anaconda3\\lib\\site-packages (from requests->transformers==4.28.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\parkchunsoo\\anaconda3\\lib\\site-packages (from requests->transformers==4.28.0) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\parkchunsoo\\anaconda3\\lib\\site-packages (from requests->transformers==4.28.0) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\parkchunsoo\\anaconda3\\lib\\site-packages (from requests->transformers==4.28.0) (2.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.28.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in c:\\users\\parkchunsoo\\anaconda3\\lib\\site-packages (0.20.1)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n",
      "     ------------------------------------- 227.6/227.6 kB 14.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\parkchunsoo\\anaconda3\\lib\\site-packages (from accelerate) (2.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\parkchunsoo\\anaconda3\\lib\\site-packages (from accelerate) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\parkchunsoo\\anaconda3\\lib\\site-packages (from accelerate) (21.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\parkchunsoo\\anaconda3\\lib\\site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\parkchunsoo\\anaconda3\\lib\\site-packages (from accelerate) (5.8.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\parkchunsoo\\anaconda3\\lib\\site-packages (from packaging>=20.0->accelerate) (3.0.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\parkchunsoo\\anaconda3\\lib\\site-packages (from torch>=1.6.0->accelerate) (3.3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\parkchunsoo\\anaconda3\\lib\\site-packages (from torch>=1.6.0->accelerate) (2.11.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\parkchunsoo\\anaconda3\\lib\\site-packages (from torch>=1.6.0->accelerate) (3.10.0.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\parkchunsoo\\anaconda3\\lib\\site-packages (from torch>=1.6.0->accelerate) (2.6.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\parkchunsoo\\anaconda3\\lib\\site-packages (from torch>=1.6.0->accelerate) (1.9)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\parkchunsoo\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.6.0->accelerate) (1.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\parkchunsoo\\anaconda3\\lib\\site-packages (from sympy->torch>=1.6.0->accelerate) (1.2.1)\n",
      "Installing collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.20.1\n",
      "    Uninstalling accelerate-0.20.1:\n",
      "      Successfully uninstalled accelerate-0.20.1\n",
      "Successfully installed accelerate-0.20.3\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uDXAxpScI3RM"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tsfQ1draI3s0"
   },
   "outputs": [],
   "source": [
    "def cleaning(s):\n",
    "    s = str(s)\n",
    "    s = re.sub('\\s\\W',' ',s)\n",
    "    s = re.sub('\\W,\\s',' ',s)\n",
    "    s = re.sub(\"\\d+\", \"\", s)\n",
    "    s = re.sub('\\s+',' ',s)\n",
    "    s = re.sub('[!@#$_]', '', s)\n",
    "    s = s.replace(\"co\",\"\")\n",
    "    s = s.replace(\"https\",\"\")\n",
    "    s = s.replace(\"[\\w*\",\" \")\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "opapKwbnI51e",
    "outputId": "4814ff29-b57e-4bf1-bac7-45e2faced611"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data/final_overall.csv\") \n",
    "df = df.dropna()\n",
    "df0 = df.loc[df['label']==0] #delivery\n",
    "df1 = df.loc[df['label']==1] #size\n",
    "df2 = df.loc[df['label']==2] #color\n",
    "df3 = df.loc[df['label']==3] #quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BiSb-pOuJJuQ"
   },
   "outputs": [],
   "source": [
    "text_data = open('Articles_delivery.txt', 'w')\n",
    "for idx, item in df0.iterrows():\n",
    "  article = cleaning(item.sentence)\n",
    "  text_data.write(article)\n",
    "text_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = open('Articles_size.txt', 'w')\n",
    "for idx, item in df1.iterrows():\n",
    "  article = cleaning(item.sentence)\n",
    "  text_data.write(article)\n",
    "text_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = open('Articles_color.txt', 'w')\n",
    "for idx, item in df2.iterrows():\n",
    "  article = cleaning(item.sentence)\n",
    "  text_data.write(article)\n",
    "text_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = open('Articles_quality.txt', 'w')\n",
    "for idx, item in df3.iterrows():\n",
    "  article = cleaning(item.sentence)\n",
    "  text_data.write(article)\n",
    "text_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "3PXkmkvTJQZw"
   },
   "outputs": [],
   "source": [
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "gNtUeTbjJRBS"
   },
   "outputs": [],
   "source": [
    "def load_dataset(file_path, tokenizer, block_size = 128):\n",
    "    dataset = TextDataset(\n",
    "        tokenizer = tokenizer,\n",
    "        file_path = file_path,\n",
    "        block_size = block_size,\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_data_collator(tokenizer, mlm = False):\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, \n",
    "        mlm=mlm,\n",
    "    )\n",
    "    return data_collator\n",
    "\n",
    "\n",
    "def train(train_file_path,model_name,\n",
    "          output_dir,\n",
    "          overwrite_output_dir,\n",
    "          per_device_train_batch_size,\n",
    "          num_train_epochs,\n",
    "          save_steps):\n",
    "  tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "  train_dataset = load_dataset(train_file_path, tokenizer)\n",
    "  data_collator = load_data_collator(tokenizer)\n",
    "\n",
    "  tokenizer.save_pretrained(output_dir)\n",
    "      \n",
    "  model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "  model.save_pretrained(output_dir)\n",
    "\n",
    "  training_args = TrainingArguments(\n",
    "          output_dir=output_dir,\n",
    "          overwrite_output_dir=overwrite_output_dir,\n",
    "          per_device_train_batch_size=per_device_train_batch_size,\n",
    "          num_train_epochs=num_train_epochs,\n",
    "      )\n",
    "\n",
    "  trainer = Trainer(\n",
    "          model=model,\n",
    "          args=training_args,\n",
    "          data_collator=data_collator,\n",
    "          train_dataset=train_dataset,\n",
    "  )\n",
    "      \n",
    "  trainer.train()\n",
    "  trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "W6O-HYS2JUYh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcharlie39653965\u001b[0m (\u001b[33mtextmining\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\ParkChunSoo\\Desktop\\ÌÖçÏä§Ìä∏ÎßàÏù¥Îãù\\TextMining_TeamProject\\wandb\\run-20230613_172617-yst8m1r0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/textmining/huggingface/runs/yst8m1r0' target=\"_blank\">morning-cosmos-10</a></strong> to <a href='https://wandb.ai/textmining/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/textmining/huggingface' target=\"_blank\">https://wandb.ai/textmining/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/textmining/huggingface/runs/yst8m1r0' target=\"_blank\">https://wandb.ai/textmining/huggingface/runs/yst8m1r0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:39, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# you need to set parameters \n",
    "train_file_path = \"Articles_delivery.txt\"\n",
    "model_name = 'gpt2'\n",
    "output_dir = '/fine_tuned_gpt2_fashion_delivery'\n",
    "overwrite_output_dir = False\n",
    "per_device_train_batch_size = 8\n",
    "num_train_epochs = 1\n",
    "save_steps = 500\n",
    "\n",
    "# It takes about 30 minutes to train in colab.\n",
    "train(\n",
    "    train_file_path=train_file_path,\n",
    "    model_name=model_name,\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=overwrite_output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    save_steps=save_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1024' max='1024' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1024/1024 53:47, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.574200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.397000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# you need to set parameters \n",
    "train_file_path = \"Articles_size.txt\"\n",
    "model_name = 'gpt2'\n",
    "output_dir = '/fine_tuned_gpt2_fashion_size'\n",
    "overwrite_output_dir = False\n",
    "per_device_train_batch_size = 8\n",
    "num_train_epochs = 1\n",
    "save_steps = 500\n",
    "\n",
    "# It takes about 30 minutes to train in colab.\n",
    "train(\n",
    "    train_file_path=train_file_path,\n",
    "    model_name=model_name,\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=overwrite_output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    save_steps=save_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ParkChunSoo\\anaconda3\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "C:\\Users\\ParkChunSoo\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='118' max='118' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [118/118 06:04, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# you need to set parameters \n",
    "train_file_path = \"Articles_color.txt\"\n",
    "model_name = 'gpt2'\n",
    "output_dir = '/fine_tuned_gpt2_fashion_color'\n",
    "overwrite_output_dir = False\n",
    "per_device_train_batch_size = 8\n",
    "num_train_epochs = 1\n",
    "save_steps = 500\n",
    "\n",
    "# It takes about 30 minutes to train in colab.\n",
    "train(\n",
    "    train_file_path=train_file_path,\n",
    "    model_name=model_name,\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=overwrite_output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    save_steps=save_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1238' max='1238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1238/1238 1:05:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.830300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.693800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# you need to set parameters \n",
    "train_file_path = \"Articles_quality.txt\"\n",
    "model_name = 'gpt2'\n",
    "output_dir = '/fine_tuned_gpt2_fashion_quality'\n",
    "overwrite_output_dir = False\n",
    "per_device_train_batch_size = 8\n",
    "num_train_epochs = 1\n",
    "save_steps = 500\n",
    "\n",
    "# It takes about 30 minutes to train in colab.\n",
    "train(\n",
    "    train_file_path=train_file_path,\n",
    "    model_name=model_name,\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=overwrite_output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    save_steps=save_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "Q7rziNRuKAbj"
   },
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel, GPT2TokenizerFast, GPT2Tokenizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "zXcfmjWnKB9I"
   },
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_tokenizer(tokenizer_path):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def generate_text(sequence, max_length, path):\n",
    "    model_path = path\n",
    "    model = load_model(model_path)\n",
    "    tokenizer = load_tokenizer(model_path)\n",
    "    ids = tokenizer.encode(f'{sequence}', return_tensors='pt')\n",
    "    final_outputs = model.generate(\n",
    "        ids,\n",
    "        do_sample=True,\n",
    "        max_length=max_length,\n",
    "        pad_token_id=model.config.eos_token_id,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "    )\n",
    "    return str(tokenizer.decode(final_outputs[0], skip_special_tokens=True))\n",
    "\n",
    "def extract_sentences(text):\n",
    "    sentences = re.findall(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yhZYIeJNKG4W",
    "outputId": "111e7135-2bf5-4047-9cf1-b5f19c07a54b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Î∞∞ÏÜ°, ÏÇ¨Ïù¥Ï¶à, ÏÉâÏÉÅ, ÌÄÑÎ¶¨Ìã∞ Ï§ë ÏÑ†ÌÉùÌïòÏãúÏò§ : Î∞∞ÏÜ°\n",
      " \n",
      "   ¬†- you got to choose. The price was excellent, I ordered a bottle which I had with me when it came out for sale, but I bought the bottle in the event this arrived.The package was super large, the delivery\n",
      " The price was excellent, I ordered a bottle which I had with me when it came out for sale, but I bought the bottle in the event this arrived.\n"
     ]
    }
   ],
   "source": [
    "print(\"Î∞∞ÏÜ°, ÏÇ¨Ïù¥Ï¶à, ÏÉâÏÉÅ, ÌÄÑÎ¶¨Ìã∞ Ï§ë ÏÑ†ÌÉùÌïòÏãúÏò§ : \", end='')\n",
    "path = input() #fine_tuned_gpt2_fahsion_delivery\n",
    "sequence = input() # oil price\n",
    "max_len = 50 #int(input()) # 20\n",
    "if path == \"Î∞∞ÏÜ°\":\n",
    "    path = \"/fine_tuned_gpt2_fashion_delivery\"\n",
    "elif path == \"ÏÇ¨Ïù¥Ï¶à\":\n",
    "    path = \"/fine_tuned_gpt2_fashion_size\"\n",
    "elif path == \"ÏÉâÏÉÅ\":\n",
    "    path = \"/fine_tuned_gpt2_fashion_color\"\n",
    "else:\n",
    "    path = \"/fine_tuned_gpt2_fashion_quality\"\n",
    "res = generate_text(sequence, max_len, path)\n",
    "print(res)\n",
    "print(res.split(sep = '.')[1]+\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dt = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"Good\"\n",
    "max_len = 50\n",
    "path = \"/fine_tuned_gpt2_fashion_delivery\"\n",
    "temp = []\n",
    "for i in range(300):\n",
    "    res = generate_text(sequence, max_len, path)\n",
    "    res = res.split(sep = '.')[1]+\".\"\n",
    "    print(res)\n",
    "    temp.append(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "bR5i13DFJ5KF"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_num</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>95</td>\n",
       "      <td>It arrived WEEKS later and has absolutely no s...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>146</td>\n",
       "      <td>We can see on the label it came directly from ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>146</td>\n",
       "      <td>The items we ordered from America all got here...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>185</td>\n",
       "      <td>i tried to return it but it will cost me twice...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>340</td>\n",
       "      <td>Only positive was that it arrived well before ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174913</th>\n",
       "      <td>109595</td>\n",
       "      <td>Super fast delivery.</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175034</th>\n",
       "      <td>109701</td>\n",
       "      <td>They did arrive in the time frame that they to...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175051</th>\n",
       "      <td>109716</td>\n",
       "      <td>Product arrived very fast.</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175261</th>\n",
       "      <td>109880</td>\n",
       "      <td>Fast delivery A+++++</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175350</th>\n",
       "      <td>109943</td>\n",
       "      <td>Mine arrived earlier than expected, which is a...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1304 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        column_num                                           sentence  label  \\\n",
       "148             95  It arrived WEEKS later and has absolutely no s...      0   \n",
       "226            146  We can see on the label it came directly from ...      0   \n",
       "227            146  The items we ordered from America all got here...      0   \n",
       "281            185  i tried to return it but it will cost me twice...      0   \n",
       "506            340  Only positive was that it arrived well before ...      0   \n",
       "...            ...                                                ...    ...   \n",
       "174913      109595                               Super fast delivery.      0   \n",
       "175034      109701  They did arrive in the time frame that they to...      0   \n",
       "175051      109716                         Product arrived very fast.      0   \n",
       "175261      109880                               Fast delivery A+++++      0   \n",
       "175350      109943  Mine arrived earlier than expected, which is a...      0   \n",
       "\n",
       "        overall  \n",
       "148           1  \n",
       "226           1  \n",
       "227           1  \n",
       "281           1  \n",
       "506           1  \n",
       "...         ...  \n",
       "174913        5  \n",
       "175034        5  \n",
       "175051        5  \n",
       "175261        5  \n",
       "175350        5  \n",
       "\n",
       "[1304 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['label']==0]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
